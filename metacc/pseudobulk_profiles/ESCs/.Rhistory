ard_z=object@model_options$ard_z
)
# Build the model
biofam_entrypoint$build()
# Run the model
biofam_entrypoint$run()
# Save the model as an hdf5 file
biofam_entrypoint$save(
outfile = dir_options$outfile
)
# Load the trained model
object <- load_model(dir_options$outfile, object)
return(object)
}
model <- run_biofam(object = object, dir_options = dir_options)
model_opts
# Create bioFAM object
object <- create_biofam(data)
dir_options <- list(
"data_dir" = tempdir(), # Folder to store the input matrices as .txt files, it can be a simple temporary folder
"outfile" = "/Users/ricard/Ecker_2017/biofam/out/model.hdf5" # Output file of the model (use hdf5 extension)
)
data_opts <- get_default_data_options(object)
data_opts$center_features <- FALSE
data_opts$center_features_per_group <- TRUE
model_opts <- get_default_model_options(object)
model_opts$learn_intercept <- FALSE
model_opts$num_factors <- 5
model_opts$sl_z <- TRUE
model_opts$sl_w <- TRUE
model_opts$ard_w <- TRUE
model_opts$ard_z <- FALSE
train_opts <- get_default_training_options(object)
train_opts$maxiter <- 1000
train_opts$drop_factor_threshold <- 0.0
train_opts$learn_factors <- F
train_opts$tolerance <- 0.01
object <- prepare_biofam(
object = object,
dir_options = dir_options,
data_options = data_opts,
training_options = train_opts,
model_options = model_opts
)
model <- run_biofam(object = object, dir_options = dir_options)
plot_variance_explained(model)
cor( colMeans(model@training_data$genebody_CA$human, na.rm=T), get_factors(model))
get_factors(model)
names(get_factors(model))
cor( colMeans(model@training_data$genebody_CA$human, na.rm=T), get_factors(model)$human)
cor( colMeans(model@training_data$genebody_CG$human, na.rm=T), get_factors(model)$human)
object = model
views = "all"
groups = "all"
factors = "all"
# Sanity checks
if (class(object) != "BioFAModel") stop("'object' has to be an instance of BioFAModel")
# Define views and groups
views  <- .check_and_get_views(object, views)
groups <- .check_and_get_groups(object, groups)
.infer_likelihoods <- function(object) {
likelihood <- rep(x="gaussian", times=object@dimensions$M)
names(likelihood) <- views_names(object)
for (view in views_names(object)) {
data <- get_training_data(object, view)[[1]][[1]]  # take only first group
# if (all(data %in% c(0,1,NA))) {
if (length(unique(data[!is.na(data)]))==2) {
likelihood[view] <- "bernoulli"
} else if (all(data[!is.na(data)]%%1==0)) {
likelihood[view] <- "poisson"
}
}
return(likelihood)
}
.update_old_model <- function(object) {
if (class(object) != "BioFAModel") stop("'object' has to be an instance of BioFAModel")
# Update node names
if ("SW" %in% names(object@expectations)) {
# object@model_options$schedule[object@model_options$schedule == "SW"] <- "W" # schedule is depreciated from model_options
names(object@expectations)[names(object@expectations) == "SW"] <- "W"
colnames(object@training_stats$elbo_terms)[colnames(object@training_stats$elbo_terms)=="SW"] <- "W"
}
if ("SZ" %in% names(object@expectations)) {
names(object@expectations)[names(object@expectations) == "SZ"] <- "Z"
colnames(object@training_stats$elbo_terms)[colnames(object@training_stats$elbo_terms)=="SZ"] <- "Z"
}
# Update expectations
if (is.list(object@expectations$Z[[1]]) & ("E" %in% names(object@expectations$Z[[1]]))) {
# Multi-view nodes
for (m in views_names(object)) {
for (node in object@model_options$nodes$multiview_node) {
if (node %in% names(object@expectations)){
object@expectations[[node]][[m]] <- object@expectations[[node]][[m]]$E
}
}
}
# Multi-group nodes
for (p in groups_names(object)) {
for (node in object@model_options$nodes$multigroup_nodes) {
if (node %in% names(object@expectations)){
object@expectations[[node]][[p]] <- object@expectations[[node]][[p]]$E
}
}
}
# Multi-view & multi-group nodes
for (m in views_names(object)) {
for (p in groups_names(object)) {
for (node in object@model_options$nodes$twodim_nodes) {
object@expectations[[node]][[m]][[p]] <- object@expectations[[node]][[m]][[p]]$E
}
}
}
}
# update learn_mean to learn_intercept
if ("learn_mean" %in% names(object@model_options)) {
tmp <- names(object@model_options)
tmp[tmp=="learn_mean"] <- "learn_intercept"
names(object@model_options) <- tmp
}
object@model_options$learn_intercept <- as.logical(object@model_options$learn_intercept)
# Set the status as trained if it wasn't set before
if((!.hasSlot(object, "status")) | (length(object@status) == 0))
object@status <- "trained"
return(object)
}
# Set view names and group names for nested list objects (e.g. Y)
.name_views_and_groups <- function(nested_list, view_names, group_names) {
names(nested_list) <- view_names
for (view in view_names) { names(nested_list[[view]]) <- group_names }
nested_list
}
# Function to find factors that act like an intercept term for the sample,
# which means that they capture global mean effects
find_intercept_factors <- function(object, cor_threshold = 0.8) {
# Sanity checks
if (class(object) != "BioFAModel") stop("'object' has to be an instance of BioFAModel")
data <- get_training_data(object)
factors <- get_factors(object, include_intercept = F)
r <- lapply(data, function(x) abs(cor(apply(x,2,mean),factors, use="complete.obs")))
for (i in names(r)) {
if (any(r[[i]]>cor_threshold))
cat(paste0("Warning: factor ",which(r[[i]]>cor_threshold)," is capturing a size factor effect in ", i, " view, which indicates that input data might not be properly normalised...\n"))
}
}
subset_augment <- function(mat, pats) {
pats <- unique(pats)
mat <- t(mat)
aug_mat <- matrix(NA, ncol=ncol(mat), nrow=length(pats))
aug_mat <- mat[match(pats,rownames(mat)),,drop=FALSE]
rownames(aug_mat) <- pats
colnames(aug_mat) <- colnames(mat)
return(t(aug_mat))
}
detect_passengers <- function(object, views = "all", groups = "all", factors = "all", r2_threshold = 0.03) {
# Sanity checks
if (class(object) != "BioFAModel") stop("'object' has to be an instance of BioFAModel")
# Define views
if (paste0(views, sep="", collapse="") == "all") {
views <- views_names(object)
} else {
stopifnot(all(views %in% views_names(object)))
}
M <- length(views)
# Define groups
if (paste0(groups, sep="", collapse="") == "all") {
groups <- groups_names(object)
} else {
stopifnot(all(groups %in% groups_names(object)))
}
H <- length(groups)
# Define factors
factors <- as.character(factors)
if (paste0(factors, collapse="") == "all") {
factors <- factors_names(object)
} else {
stopifnot(all(factors %in% factors_names(object)))
}
# Collect relevant data
Z <- get_factors(object)
# Identify factors unique to a single view by calculating relative R2 per factor
r2 <- calculate_variance_explained(object, views = views, groups = groups, factors = factors)$r2_per_factor
unique_factors <- unique(unlist(lapply(groups, function(p) names(which(rowSums(r2[[p]]>=r2_threshold)==1)) )))
# Mask samples that are unique in the unique factors
missing <- lapply(get_training_data(object, views, groups), function(views) {
lapply(views, function(group) {
names(which(apply(group,2,function(x) all(is.na(x)))))
})
})
missing <- .name_views_and_groups(missing, views_names(object), groups_names(object))
for (fctr in unique_factors) {
# view <- names(which(r2[fctr,]>=r2_threshold))
for (p in groups) {
view <- colnames(r2[[p]][,which(r2[[p]][fctr,]>=r2_threshold), drop=F])
if (!is.null(view)) {
missing_samples <- missing[[view]][[p]]
if (length(missing_samples) > 0) {
Z[[p]][missing_samples, fctr] <- rep(NA,length(missing_samples))
}
}
}
}
# Replace the latent matrix
object@expectations$Z <- Z
return(object)
}
flip_factor <- function(model, factor){
for(groupnm in names(model@expectations$Z)) {
model@expectations$Z[[groupnm]][,factor] <- - model@expectations$Z[[groupnm]][,factor]
}
for(viewnm in names(model@expectations$W)) {
model@expectations$W[[viewnm]][,factor] <- -model@expectations$W[[viewnm]][,factor]
}
return(model)
}
.check_and_get_views <- function(object, views) {
if (is.numeric(views)) {
stopifnot(all(views <= object@dimensions$M))
views_names(object)[views]
} else {
if (paste0(views, sep = "", collapse = "") == "all") {
views_names(object)
} else {
stopifnot(all(views %in% views_names(object)))
views
}
}
}
.check_and_get_groups <- function(object, groups) {
if (is.numeric(groups)) {
stopifnot(all(groups <= object@dimensions$P))
groups_names(object)[groups]
} else {
if (paste0(groups, sep = "", collapse = "") == "all") {
groups_names(object)
} else {
stopifnot(all(groups %in% groups_names(object)))
groups
}
}
}
setClass("matrix_placeholder",
slots=c(rownames = "ANY",
colnames = "ANY",
nrow     = "integer",
ncol     = "integer")
)
setMethod("rownames", "matrix_placeholder", function(x) { x@rownames })
setMethod("colnames", "matrix_placeholder", function(x) { x@colnames })
setMethod("nrow", "matrix_placeholder", function(x) { x@nrow })
setMethod("ncol", "matrix_placeholder", function(x) { x@ncol })
setReplaceMethod("rownames", signature(x = "matrix_placeholder"),
function(x, value) { x@rownames <- value; x@nrow <- length(value); x })
setReplaceMethod("colnames", signature(x = "matrix_placeholder"),
function(x, value) { x@colnames <- value; x@ncol <- length(value); x })
.create_matrix_placeholder <- function(rownames, colnames) {
mx <- new("matrix_placeholder")
mx@rownames <- rownames
mx@colnames <- colnames
mx@nrow <- length(rownames)
mx@ncol <- length(colnames)
mx
}
.rep_string <- function(times, string, collapse = "") {
paste(replicate(times, string), collapse = collapse)
}
.pad_left_with <- function(len, string, with = "") {
wlen <- nchar(with)
len  <- max(len - wlen, 0)
paste0(with, paste(replicate(len, " "), collapse = ""), string)
}
.pad_left <- function(len, string) {
.pad_left_with(len, string, with = "")
}
# Center and paste
.cpaste <- function(vals, cwidth, collapse = "") {
vals <- sapply(vals, function(e) {
e <- toString(e)
lendiff <- cwidth - nchar(e)
if (lendiff > 1) {
paste0(.rep_string(ceiling(lendiff / 2), " "),
e,
.rep_string(floor(lendiff / 2), " "))
} else {
e
}
})
paste(vals, collapse = collapse)
}
# Fancy printing method
vis <- function(object) {
stopifnot(class(object) == "BioFAModel")
if (!.hasSlot(object, "dimensions") | length(object@dimensions) == 0)
stop("Error: dimensions not defined")
if (!.hasSlot(object, "status") | length(object@status) == 0)
stop("Error: status not defined")
vis_lines <- ""
lpad <- max(sapply(views_names(object), function(v) nchar(v)))
wlim <- max(sapply(groups_names(object), function(v) nchar(v)))
igr_sp <- .rep_string(5, " ")
s <- 8             # extra lpadding shift
w <- max(8, wlim)  # width of one block (minus 2 walls)
hat    <- paste0(" ", .rep_string(w, "_"), " ")
walls  <- paste0("|", .rep_string(w, " "), "|")
ground <- paste0("|", .rep_string(w, "_"), "|")
cat("
\U2588︎\U2588︎\U2588︎\U2588︎\U2588︎     \U2588︎\U2588︎   \U2588\U2588︎\U2588︎\U2588︎\U2588︎
biofam   \U2588︎\U2588︎\U2588︎\U2588︎\U2588︎  =  \U2588︎\U2588︎ x \U2588︎\U2588︎\U2588︎\U2588︎\U2588︎
\U2588︎\U2588︎\U2588︎\U2588︎\U2588︎     \U2588︎\U2588︎
")
groups_line    <- .pad_left(lpad + s, .cpaste(groups_names(object), w+2, collapse = igr_sp))
nsamples_line  <- .pad_left(lpad + s, .cpaste(get_dimensions(object)$N, w+2, collapse = igr_sp))
vis_lines      <- c(vis_lines, groups_line, nsamples_line)
for (m in 1:length(views_names(object))) {
toprect_line   <- .pad_left(lpad + s, paste(.rep_string(get_dimensions(object)$P, hat, collapse = igr_sp)))
midrect_line   <- .pad_left(lpad + s, paste(.rep_string(get_dimensions(object)$P, walls, collapse = igr_sp)))
dfeatures_line <- .pad_left_with(lpad + s,
paste(.rep_string(get_dimensions(object)$P, walls, collapse = igr_sp)),
with = paste(c(views_names(object)[m], .cpaste(get_dimensions(object)$D[m], s)), collapse = ""))
botrect_line   <- .pad_left(lpad + s, paste(.rep_string(get_dimensions(object)$P, ground, collapse = igr_sp)))
vis_lines      <- c(vis_lines, toprect_line, midrect_line, dfeatures_line, botrect_line)
}
cat(paste(vis_lines, collapse = "\n"))
cat("\n\n")
}
# Define views and groups
views  <- .check_and_get_views(object, views)
groups <- .check_and_get_groups(object, groups)
# Define factors
if (paste0(factors, collapse="") == "all") {
factors <- factors_names(object)
} else if (is.numeric(factors)) {
factors <- factors_names(object)[factors]
} else {
stopifnot(all(factors %in% factors_names(object)))
}
K <- length(factors)
# Collect relevant expectations
W <- get_weights(object, views, factors)
Z <- get_factors(object, groups, factors)
Y <- get_expectations(object, "Y")  # for non-Gaussian likelihoods the pseudodata is considered
Y <- lapply(Y, function(x) lapply(x,t))
# Replace masked values on Z by 0 (so that they do not contribute to predictions)
for (p in groups) {
Z[[p]][is.na(Z[[p]])] <- 0
}
for (m in views) { for (p in groups) {
if (!all(colMeans(Y[[m]][[p]],na.rm=T)<1e-2,na.rm=T))
cat(sprintf("Warning: data for view %s is not centered\n",m))
}}
Y <- .name_views_and_groups(Y, views, groups)
# Calculate coefficient of determination per group and view
fvar_m <- lapply(groups, function(p)
lapply(views, function(m) {
a <- sum((Y[[m]][[p]]-tcrossprod(Z[[p]],W[[m]]))**2, na.rm=T)
# b <- sum(scale(Y[[m]][[p]], center=T, scale=F)**2, na.rm=T)
b <- sum(Y[[m]][[p]]**2, na.rm=T)
return(1 - a/b)
}
))
fvar_m
view_names(object)
views_names(object)
library(rhdf5)
file"/Users/ricard/Ecker_2017/biofam/out/model.hdf5"
file="/Users/ricard/Ecker_2017/biofam/out/model.hdf5"
sort_factors = TRUE
on_disk = FALSE
load_training_data = TRUE
# Create new bioFAModel object
if (is.null(object)) object <- new("BioFAModel")
# Sanity checks
if (.hasSlot(object, "status") & length(object@status) != 0)
if (object@status == "trained") warning("The specified object is already trained, over-writing training output with new results.")
if (.hasSlot(object, "on_disk") & (on_disk)) object@on_disk <- TRUE
# Get groups and data set names from the hdf5 file object
foo <- h5ls(file, datasetinfo = F)
foo
# Load identity of features and samples
feature_names <- h5read(file, "features")
feature_names
sample_names  <- h5read(file, "samples")
feature_groups <- foo[foo$group=="/data","name"]
sample_groups <- foo[foo$group==paste0("/data/",feature_groups[1]),"name"]
# Load data matrices
training_data <- list()
if (load_training_data) {
for (m in feature_groups) {
training_data[[m]] <- list()
for (p in sample_groups) {
if (on_disk) {
# as DelayedArrays
training_data[[m]][[p]] <- DelayedArray( HDF5ArraySeed(file, name = sprintf("data/%s/%s", m, p) ) )
} else {
# as matrices
training_data[[m]][[p]] <- h5read(file, sprintf("data/%s/%s", m, p) )
}
}
}
} else {
# Do not load matrices
n_features <- lapply(feature_names, length)
n_samples  <- lapply(sample_names, length)
for (m in feature_groups) {
training_data[[m]] <- list()
for (p in sample_groups) {
training_data[[m]][[p]] <- .create_matrix_placeholder(rownames = feature_names[[m]], colnames = sample_names[[p]])
}
}
}
training_data$genebody_CA
training_data$genebody_CA[1,]
training_data$genebody_CA$human[1,]
training_data$genebody_CA$human[1,1]
object@training_data <- training_data
expectations <- list()
node_names <- foo[foo$group=="/expectations","name"]
if ("AlphaW" %in% node_names)
expectations[["AlphaW"]] <- h5read(file, "expectations/AlphaW")
if ("AlphaZ" %in% node_names)
expectations[["AlphaZ"]] <- h5read(file, "expectations/AlphaZ")
# TO-DO: IF TAU IS EXPANDED IT SHOULD BE A DELAYEDARRAY
if ("Tau" %in% node_names)
expectations[["Tau"]] <- h5read(file, "expectations/Tau")
# RICARD: I DON'T THINK Z OR W SHOULD BE DELAYED ARRAYS, THEY SHOULD FIT IN MEMORY
if ("Z" %in% node_names)
expectations[["Z"]] <- h5read(file, "expectations/Z")
if ("SZ" %in% node_names)
expectations[["Z"]] <- h5read(file, "expectations/SZ")
if ("W" %in% node_names)
expectations[["W"]] <- h5read(file, "expectations/W")
if ("SW" %in% node_names)
expectations[["W"]] <- h5read(file, "expectations/SW")
if ("ThetaW" %in% node_names)
expectations[["ThetaW"]] <- h5read(file, "expectations/ThetaW")
if ("ThetaZ" %in% node_names)
expectations[["ThetaZ"]] <- h5read(file, "expectations/ThetaZ")
if ("Y" %in% node_names) {
expectations[["Y"]] <- list()
for (m in feature_groups) {
expectations[["Y"]][[m]] <- list()
for (p in sample_groups) {
if (on_disk) {
expectations[["Y"]][[m]][[p]] <- DelayedArray( HDF5ArraySeed(file, name=sprintf("expectations/Y/%s/%s/E", m, p)) )
} else {
expectations[["Y"]][[m]][[p]] <- h5read(file, sprintf("expectations/Y/%s/%s/E", m, p))
}
}
}
}
object@expectations <- expectations
# Specify dimensionality of the data
object@dimensions[["M"]] <- length(training_data)                           # number of views (groups of features)
object@dimensions[["P"]] <- length(training_data[[1]])                      # number of groups (groups of samples)
object@dimensions[["N"]] <- sapply(training_data[[1]], ncol)                # number of samples per sample_group
object@dimensions[["D"]] <- sapply(training_data, function(e) nrow(e[[1]])) # number of features per feature_group (view)
object@dimensions[["K"]] <- ncol(object@expectations$W[[1]]$E)              # number of factors
# Fix sample and feature names is they are null
if (is.null(sample_names))
sample_names <- lapply(object@dimensions[["N"]], function(n) paste0("sample", as.character(1:n)))
if (is.null(feature_names))
feature_names <- lapply(object@dimensions[["D"]], function(d) paste0("feature", as.character(1:d)))
feature_names
# Set feature_group names
if (is.null(names(object@training_data))) {
views_names(object) <- paste0("feature_group", as.character(1:object@dimensions[["M"]]))
} else {
views_names(object) <- names(object@training_data)
}
# Set sample_group names
if (is.null(names(object@training_data[[1]]))) {
groups_names(object) <- paste0("sample_group", as.character(1:object@dimensions[["P"]]))
} else {
groups_names(object) <- names(object@training_data[[1]])
}
object@training_data$genebody_CA$human
colnames(object@training_data$genebody_CA$human)
tryCatch( {
object@model_options <- as.list(h5read(file, 'model_options', read.attributes=T))
}, error = function(x) { print("Model opts not found, not loading it...") })
# Convert True/False Strings to logical values
for (opt in names(object@model_options)) {
if (object@model_options[opt] == "False" | object@model_options[opt] == "True") {
object@model_options[opt] <- as.logical(object@model_options[opt])
} else {
object@model_options[opt] <- object@model_options[opt]
}
}
# Define node types of the model
object@model_options$nodes <- list(multiview_nodes  = c("W", "AlphaW", "ThetaW", "SigmaAlphaW"),
multigroup_nodes = c("Z", "AlphaZ", "ThetaZ", "SigmaZ"),
twodim_nodes     = c("Y", "Tau"))
# Set sample, feature, and factor names for the data and all the expectations
samples_names(object)  <- sample_names
features_names(object) <- feature_names
factors_names(object)  <- paste0("Factor", as.character(1:object@dimensions[["K"]]))
object@training_data$genebody_CA
object@training_data$genebody_CA[1,1]
object@training_data$genebody_CA$human[1,1]
object@training_data$genebody_CA$human[1:2,1:2]
met_dt[id=="ENSG00000001561" & sample=="Pool_1_AD008"]
object@training_data$genebody_CG$human[1:2,1:2]
object@training_data$genebody_CG$human["ENSG00000001561","Pool_1_AD008"]
object@training_data$genebody_CG$human["ENSG00000001561",]
object@training_data$genebody_CA$human["ENSG00000001561",]
object@training_data$genebody_CA$human["ENSG00000001561","Pool_1_AD008"]
met_dt[id=="ENSG00000001561" & sample=="Pool_1_AD008"]
object@training_data$genebody_CC$human["ENSG00000001561","Pool_1_AD008"]
object@training_data$genebody_CT$human["ENSG00000001561","Pool_1_AD008"]
object@training_data$genebody_CG$human["ENSG00000001561","Pool_1_AD008"]
data[id=="ENSG00000001561" & sample=="Pool_1_AD008"]
data[feature=="ENSG00000001561" & sample=="Pool_1_AD008"]
object@training_data$genebody_CA$human["ENSG00000001561","Pool_1_AD008"]
